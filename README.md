**Next-Word Prediction using LSTM#**

**Description:**
This project demonstrates a deep learning model for next-word prediction using an LSTM (Long Short-Term Memory) network. The model is trained on text data, where it learns to predict the next word in a sequence based on the previous words. The architecture includes an LSTM layer followed by a dense layer with a softmax activation function to output probabilities for the next word. The training process utilizes one-hot encoding to represent words and sequences.

**Features:**
- Text preprocessing with tokenization.
- Sequence generation for training the model.
- LSTM-based architecture for sequential data prediction.
- One-hot encoding of words for input and output layers.
- Word completion and prediction functions.
  
**Technologies Used:**
- Keras for neural network implementation.
- NLTK for text tokenization.
- Python libraries such as NumPy and Pickle for data handling.
  
**Usage:**
After training the model, users can input a sequence of text, and the model will predict the next word based on the learned patterns from the training data.

Feel free to clone the repo and try the prediction on different text inputs!
